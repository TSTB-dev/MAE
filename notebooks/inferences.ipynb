{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Optional, Any, List, Tuple, Dict, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torchvision import utils as vutils\n",
    "from torchvision import transforms\n",
    "\n",
    "import models_mae\n",
    "from dataset import LOCODataModule, ImageNetTransforms\n",
    "from models_mae import MaskedAutoencoderViT\n",
    "from method import MAEMethod\n",
    "from params import MAEParams\n",
    "from common import ImageLogCallback, prepare_model, to_rgb_from_tensor, get_patches_containing_bbox_1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_embed.shape: torch.Size([24, 196, 768])\n",
      "batch_embed[0] tensor([[ 0.4467, -0.2411, -1.3649,  ..., -0.6264,  0.8555, -0.9336],\n",
      "        [-0.3862, -0.9999, -0.8738,  ..., -0.7636, -1.8462, -1.5139],\n",
      "        [-0.7923,  1.6795, -0.2511,  ..., -0.5125, -0.0260, -0.9999],\n",
      "        ...,\n",
      "        [ 1.4510,  1.2476,  1.6363,  ..., -1.1198, -1.5834, -1.0546],\n",
      "        [ 1.3579, -0.2066, -0.3293,  ..., -0.3328,  0.5646,  0.1232],\n",
      "        [-1.7694,  0.9908,  0.7270,  ..., -0.6524, -0.2391, -1.9545]])\n",
      "-----\n",
      "\n",
      "batch_bbox.shape: torch.Size([24, 196])\n",
      "batch_bbox[0] tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1])\n",
      "-----\n",
      "\n",
      "length of batch_indices: 24\n",
      "batch_indices[0]: tensor([  0,   1,   7,   8,  12,  14,  16,  18,  19,  25,  31,  33,  36,  40,\n",
      "         41,  42,  44,  46,  48,  50,  56,  58,  60,  64,  66,  67,  68,  70,\n",
      "         76,  78,  79,  82,  83,  89,  90,  91,  92,  94,  95,  97,  98, 101,\n",
      "        102, 103, 104, 105, 106, 107, 108, 110, 113, 114, 118, 124, 126, 128,\n",
      "        130, 131, 136, 137, 139, 140, 141, 144, 145, 146, 148, 151, 153, 156,\n",
      "        157, 158, 159, 163, 167, 170, 172, 175, 177, 179, 180, 183, 184, 185,\n",
      "        189, 191, 193, 195])\n",
      "-----\n",
      "\n",
      "masked_batch_embed.shape: torch.Size([24, 196, 768])\n",
      "masked_batch_embed[0] tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.7923,  1.6795, -0.2511,  ..., -0.5125, -0.0260, -0.9999],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.3579, -0.2066, -0.3293,  ..., -0.3328,  0.5646,  0.1232],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# batch_embedding: (BN, L, D)\n",
    "batch_embed = torch.randn(24, 196, 768)\n",
    "print(f\"batch_embed.shape: {batch_embed.shape}\")\n",
    "print(f\"batch_embed[0]\", batch_embed[0])\n",
    "print(\"-----\\n\")\n",
    "# batch_bbox: (BN, L)\n",
    "batch_bbox = torch.randint(0, 2, (24, 196))\n",
    "print(f\"batch_bbox.shape: {batch_bbox.shape}\")\n",
    "print(f\"batch_bbox[0]\", batch_bbox[0])\n",
    "\n",
    "zero_vector = torch.zeros(768)\n",
    "print(\"-----\\n\")\n",
    "# convert batch_bbox to indices\n",
    "batch_indices = [torch.where(bbox == 1)[0] for bbox in batch_bbox]\n",
    "print(f\"length of batch_indices: {len(batch_indices)}\")\n",
    "print(f\"batch_indices[0]: {batch_indices[0]}\")\n",
    "\n",
    "print(\"-----\\n\")\n",
    "# mask out the embeddings\n",
    "for i, indices in enumerate(batch_indices):\n",
    "    batch_embed[i, indices, :] = 0.\n",
    "print(f\"masked_batch_embed.shape: {batch_embed.shape}\")\n",
    "print(f\"masked_batch_embed[0]\", batch_embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_mask.shape: torch.Size([24, 8, 196])\n",
      "-----\n",
      "\n",
      "attn_mask.shape: torch.Size([24, 8, 196, 196])\n",
      "attn_mask[0, 0] tensor([[1, 1, 0,  ..., 1, 0, 1],\n",
      "        [1, 1, 0,  ..., 1, 0, 1],\n",
      "        [1, 1, 0,  ..., 1, 0, 1],\n",
      "        ...,\n",
      "        [1, 1, 0,  ..., 1, 0, 1],\n",
      "        [1, 1, 0,  ..., 1, 0, 1],\n",
      "        [1, 1, 0,  ..., 1, 0, 1]])\n",
      "-----\n",
      "\n",
      "attn_mask.shape: torch.Size([24, 8, 196, 196])\n",
      "attn_mask[0, 0] tensor([[-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
      "         -0.0000e+00, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
      "         -0.0000e+00, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
      "         -0.0000e+00, -1.0000e+09],\n",
      "        ...,\n",
      "        [-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
      "         -0.0000e+00, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
      "         -0.0000e+00, -1.0000e+09],\n",
      "        [-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
      "         -0.0000e+00, -1.0000e+09]])\n"
     ]
    }
   ],
   "source": [
    "# self attention mask (BN, H, L, L)\n",
    "\n",
    "# expand head dimension\n",
    "attn_mask = batch_bbox.unsqueeze(1).repeat(1, 8, 1)\n",
    "print(f\"attn_mask.shape: {attn_mask.shape}\")\n",
    "print(\"-----\\n\")\n",
    "# expand L dimension\n",
    "attn_mask = attn_mask.unsqueeze(-2).repeat(1, 1, 196, 1)\n",
    "print(f\"attn_mask.shape: {attn_mask.shape}\")\n",
    "print(f\"attn_mask[0, 0]\", attn_mask[0, 0])\n",
    "print(\"-----\\n\")\n",
    "# multiply by -1e9 (Greatly negative value)\n",
    "attn_mask = attn_mask * -1e9\n",
    "print(f\"attn_mask.shape: {attn_mask.shape}\")\n",
    "print(f\"attn_mask[0, 0]\", attn_mask[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = MAEParams()\n",
    "ckpt_file = \"/home/dl/takamagahara/hutodama/MAE/src/wandb/run-20230807_101801-kuia1oij/ckpt/epoch=499-step=2500.ckpt\"\n",
    "img_transforms = ImageNetTransforms(input_resolution=params.resolution)\n",
    "datamodule = LOCODataModule(\n",
    "    data_root=params.data_root,\n",
    "    category=params.category,\n",
    "    input_resolution=params.resolution,\n",
    "    img_transforms=img_transforms,\n",
    "    batch_size=params.batch_size,\n",
    "    num_workers=params.num_workers,\n",
    ")\n",
    "\n",
    "model = getattr(models_mae, params.arch)()\n",
    "\n",
    "method = MAEMethod(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    params=params,\n",
    ")\n",
    "\n",
    "method.load_state_dict(torch.load(ckpt_file)['state_dict'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "method.eval()\n",
    "method.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "datamodule.setup(stage='test')\n",
    "train_dataset = datamodule.train_dataset\n",
    "test_dataset = datamodule.test_dataset\n",
    "valid_dataset = datamodule.valid_dataset\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "valid_dataloader = datamodule.val_dataloader()\n",
    "\n",
    "train_files = datamodule.train_dataset.files\n",
    "test_files = datamodule.test_dataset.files\n",
    "\n",
    "labels = test_dataloader.dataset.labels\n",
    "abnormal_indices = [idx for idx, label in enumerate(labels) if label == 1]\n",
    "normal_indices = [idx for idx, label in enumerate(labels) if label == 0]\n",
    "\n",
    "# indices of the logical anomalies and structural anomalies\n",
    "log_indices = [idx for idx, file in enumerate(test_dataloader.dataset.files) if 'logical' in str(file)]\n",
    "str_indices = [idx for idx, file in enumerate(test_dataloader.dataset.files) if 'structural' in str(file)]\n",
    "\n",
    "# sample anomal images\n",
    "num_samples = 5 * 3\n",
    "perm = torch.randperm(len(abnormal_indices))[:num_samples]\n",
    "indices = [abnormal_indices[idx] for idx in perm]\n",
    "sample_files = [test_files[idx] for idx in indices]\n",
    "\n",
    "fig, axes = plt.subplots(num_samples // 5, 5, figsize=(20, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img = Image.open(sample_files[i])\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "\n",
    "def inference(model: MaskedAutoencoderViT, inputs: Tensor):\n",
    "    \"\"\"Inference function.\n",
    "\n",
    "    Args:\n",
    "        model (MaskedAutoencoderViT): model\n",
    "        inputs (Tensor): inputs batch (B, C, H, W)\n",
    "    \"\"\"\n",
    "    loss, pred, mask = model(inputs)\n",
    "    return loss, pred, mask\n",
    "\n",
    "def get_batched_inputs(dataset, indices: List):\n",
    "    assert len(dataset[0]) == 2, \"dataset must return (image, label) tuple.\"\n",
    "    selected_imgs = [dataset[idx][0] for idx in indices]\n",
    "    selected_imgs = torch.stack(selected_imgs, dim=0)\n",
    "    return selected_imgs\n",
    "\n",
    "def make_grid_results(model: MaskedAutoencoderViT, inputs: Tensor, params: Any, mask_ratio: float = 0.75, \\\n",
    "    mask_indices = None):\n",
    "    \"\"\"reformat results to a grid.\"\"\"\n",
    "    \n",
    "    if params.gpus > 0:\n",
    "        batch = inputs.to(device)\n",
    "    _, pred, mask = model(batch, mask_ratio=mask_ratio, mask_indices=mask_indices)  # pred: (B, L, p*p*3), mask: (B, L)\n",
    "    \n",
    "    patch_size = model.patch_embed.patch_size[0]\n",
    "    mask = mask.detach()  # -> (B, L)\n",
    "    print(mask.shape)\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, patch_size**2 * 3)  # -> (B, L, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # -> (B, 3, H, W)\n",
    "    \n",
    "    pred = model.unpatchify(pred)  # -> (B, 3, H, W)\n",
    "    \n",
    "    # mask input images.\n",
    "    im_masked = batch * (1 - mask) # -> (B, 3, H, W)\n",
    "    im_paste = im_masked + pred * mask # -> (B, 3, H, W)\n",
    "    \n",
    "    # convert tensor to rgb format.\n",
    "    images = to_rgb_from_tensor(batch.cpu()) # -> (B, 3, H, W)\n",
    "    im_masked = to_rgb_from_tensor(im_masked.cpu())  # -> (B, 3, H, W)\n",
    "    im_paste = to_rgb_from_tensor(im_paste.cpu())  # -> (B, 3, H, W)\n",
    "    \n",
    "    # combine images in a way so we can display all outputs in one grid.\n",
    "    out = torch.cat([images.unsqueeze(1), im_masked.unsqueeze(1), im_paste.unsqueeze(1)], dim=1)  # -> (B, 3, 3, H, W)\n",
    "    out = out.view(-1, *out.shape[2:])  # -> (3*B, 3, H, W)\n",
    "    \n",
    "    images = vutils.make_grid(\n",
    "            out.cpu(),\n",
    "            nrows=out.shape[0] // 3,\n",
    "            ncols=3,\n",
    "            normalize=False, \n",
    "    )\n",
    "    # images: (3, H, W)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = get_batched_inputs(test_dataset, indices)\n",
    "grid_images = make_grid_results(method.mae, inputs, params, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.imshow(grid_images.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the modified function\n",
    "bbox = (50, 50, 70, 70)\n",
    "patch_size = (16, 16)\n",
    "img_size = (224, 224)\n",
    "mask_indices = get_patches_containing_bbox_1D(bbox, patch_size, img_size)\n",
    "print(mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_images = make_grid_results(method.mae, inputs, params, 0.75, mask_indices=mask_indices)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.imshow(grid_images.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interactive, IntSlider, HBox, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Display the image with initial bbox\n",
    "def display_bbox(x, y, width, height):\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "    ax.imshow(img)\n",
    "    rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders for bbox coordinates and size\n",
    "x_slider = IntSlider(min=0, max=img.shape[1], step=1, value=0, description='x')\n",
    "y_slider = IntSlider(min=0, max=img.shape[0], step=1, value=0, description='y')\n",
    "width_slider = IntSlider(min=0, max=img.shape[1], step=1, value=50, description='width')\n",
    "height_slider = IntSlider(min=0, max=img.shape[0], step=1, value=50, description='height')\n",
    "\n",
    "# Combine the sliders with the display function\n",
    "interactive_plot = interactive(display_bbox, x=x_slider, y=y_slider, width=width_slider, height=height_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
