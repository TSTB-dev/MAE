{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Optional, Any, List, Tuple, Dict, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torchvision import utils as vutils\n",
    "from torchvision import transforms\n",
    "\n",
    "import models_mae\n",
    "from dataset import LOCODataModule, ImageNetTransforms\n",
    "from models_mae import MaskedAutoencoderViT\n",
    "from method import MAEMethod\n",
    "from params import MAEParams\n",
    "from common import ImageLogCallback, prepare_model, to_rgb_from_tensor, get_patches_containing_bbox_1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = MAEParams()\n",
    "ckpt_file = \"/home/dl/takamagahara/hutodama/MAE/src/wandb/run-20230807_101801-kuia1oij/ckpt/epoch=499-step=2500.ckpt\"\n",
    "img_transforms = ImageNetTransforms(input_resolution=params.resolution)\n",
    "datamodule = LOCODataModule(\n",
    "    data_root=params.data_root,\n",
    "    category=params.category,\n",
    "    input_resolution=params.resolution,\n",
    "    img_transforms=img_transforms,\n",
    "    batch_size=params.batch_size,\n",
    "    num_workers=params.num_workers,\n",
    ")\n",
    "\n",
    "model = getattr(models_mae, params.arch)()\n",
    "\n",
    "method = MAEMethod(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    params=params,\n",
    ")\n",
    "\n",
    "method.load_state_dict(torch.load(ckpt_file)['state_dict'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "method.eval()\n",
    "method.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "datamodule.setup(stage='test')\n",
    "train_dataset = datamodule.train_dataset\n",
    "test_dataset = datamodule.test_dataset\n",
    "valid_dataset = datamodule.valid_dataset\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "valid_dataloader = datamodule.val_dataloader()\n",
    "\n",
    "train_files = datamodule.train_dataset.files\n",
    "test_files = datamodule.test_dataset.files\n",
    "\n",
    "labels = test_dataloader.dataset.labels\n",
    "abnormal_indices = [idx for idx, label in enumerate(labels) if label == 1]\n",
    "normal_indices = [idx for idx, label in enumerate(labels) if label == 0]\n",
    "\n",
    "# indices of the logical anomalies and structural anomalies\n",
    "log_indices = [idx for idx, file in enumerate(test_dataloader.dataset.files) if 'logical' in str(file)]\n",
    "str_indices = [idx for idx, file in enumerate(test_dataloader.dataset.files) if 'structural' in str(file)]\n",
    "\n",
    "# sample anomal images\n",
    "num_samples = 5 * 3\n",
    "perm = torch.randperm(len(abnormal_indices))[:num_samples]\n",
    "indices = [abnormal_indices[idx] for idx in perm]\n",
    "sample_files = [test_files[idx] for idx in indices]\n",
    "\n",
    "fig, axes = plt.subplots(num_samples // 5, 5, figsize=(20, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img = Image.open(sample_files[i])\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "\n",
    "def inference(model: MaskedAutoencoderViT, inputs: Tensor):\n",
    "    \"\"\"Inference function.\n",
    "\n",
    "    Args:\n",
    "        model (MaskedAutoencoderViT): model\n",
    "        inputs (Tensor): inputs batch (B, C, H, W)\n",
    "    \"\"\"\n",
    "    loss, pred, mask = model(inputs)\n",
    "    return loss, pred, mask\n",
    "\n",
    "def get_batched_inputs(dataset, indices: List):\n",
    "    assert len(dataset[0]) == 2, \"dataset must return (image, label) tuple.\"\n",
    "    selected_imgs = [dataset[idx][0] for idx in indices]\n",
    "    selected_imgs = torch.stack(selected_imgs, dim=0)\n",
    "    return selected_imgs\n",
    "\n",
    "def make_grid_results(model: MaskedAutoencoderViT, inputs: Tensor, params: Any, mask_ratio: float = 0.75, \\\n",
    "    mask_indices = None):\n",
    "    \"\"\"reformat results to a grid.\"\"\"\n",
    "    \n",
    "    if params.gpus > 0:\n",
    "        batch = inputs.to(device)\n",
    "    _, pred, mask = model(batch, mask_ratio=mask_ratio, mask_indices=mask_indices)  # pred: (B, L, p*p*3), mask: (B, L)\n",
    "    \n",
    "    patch_size = model.patch_embed.patch_size[0]\n",
    "    mask = mask.detach()  # -> (B, L)\n",
    "    print(mask.shape)\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, patch_size**2 * 3)  # -> (B, L, p*p*3)\n",
    "    mask = model.unpatchify(mask)  # -> (B, 3, H, W)\n",
    "    \n",
    "    pred = model.unpatchify(pred)  # -> (B, 3, H, W)\n",
    "    \n",
    "    # mask input images.\n",
    "    im_masked = batch * (1 - mask) # -> (B, 3, H, W)\n",
    "    im_paste = im_masked + pred * mask # -> (B, 3, H, W)\n",
    "    \n",
    "    # convert tensor to rgb format.\n",
    "    images = to_rgb_from_tensor(batch.cpu()) # -> (B, 3, H, W)\n",
    "    im_masked = to_rgb_from_tensor(im_masked.cpu())  # -> (B, 3, H, W)\n",
    "    im_paste = to_rgb_from_tensor(im_paste.cpu())  # -> (B, 3, H, W)\n",
    "    \n",
    "    # combine images in a way so we can display all outputs in one grid.\n",
    "    out = torch.cat([images.unsqueeze(1), im_masked.unsqueeze(1), im_paste.unsqueeze(1)], dim=1)  # -> (B, 3, 3, H, W)\n",
    "    out = out.view(-1, *out.shape[2:])  # -> (3*B, 3, H, W)\n",
    "    \n",
    "    images = vutils.make_grid(\n",
    "            out.cpu(),\n",
    "            nrows=out.shape[0] // 3,\n",
    "            ncols=3,\n",
    "            normalize=False, \n",
    "    )\n",
    "    # images: (3, H, W)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = get_batched_inputs(test_dataset, indices)\n",
    "grid_images = make_grid_results(method.mae, inputs, params, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.imshow(grid_images.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the modified function\n",
    "bbox = (50, 50, 70, 70)\n",
    "patch_size = (16, 16)\n",
    "img_size = (224, 224)\n",
    "mask_indices = get_patches_containing_bbox_1D(bbox, patch_size, img_size)\n",
    "print(mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_images = make_grid_results(method.mae, inputs, params, 0.75, mask_indices=mask_indices)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "ax.imshow(grid_images.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interactive, IntSlider, HBox, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# Display the image with initial bbox\n",
    "def display_bbox(x, y, width, height):\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "    ax.imshow(img)\n",
    "    rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Create sliders for bbox coordinates and size\n",
    "x_slider = IntSlider(min=0, max=img.shape[1], step=1, value=0, description='x')\n",
    "y_slider = IntSlider(min=0, max=img.shape[0], step=1, value=0, description='y')\n",
    "width_slider = IntSlider(min=0, max=img.shape[1], step=1, value=50, description='width')\n",
    "height_slider = IntSlider(min=0, max=img.shape[0], step=1, value=50, description='height')\n",
    "\n",
    "# Combine the sliders with the display function\n",
    "interactive_plot = interactive(display_bbox, x=x_slider, y=y_slider, width=width_slider, height=height_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
